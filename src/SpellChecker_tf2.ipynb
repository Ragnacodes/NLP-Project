{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdd6Bmh309uq"
      },
      "source": [
        "# Creating a Spell Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tKuMb3iH09uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcee259-4dd8-42f1-d595-eed1c4adcc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import namedtuple\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "import time\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z5EZXJ509ux"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iXko_qv7pwO",
        "outputId": "4245880a-524a-4fa8-abad-7dfc58b2c5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/NPL-Project-Data/dataset.csv')\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "0jQWdMbc7zrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "g8WXTiSq7-ti",
        "outputId": "399db348-1807-4620-97cb-d23065cb9db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      noise_sentence  \\\n",
              "0  AYandon abandoned or abandonment may reffr to ...   \n",
              "1  Abandoned an episode of the TV secies Smallvil...   \n",
              "2  Thy include common abilities like walking and ...   \n",
              "3  Abilities aer intelligent powers they are guid...   \n",
              "4  They are closely related to but not identical ...   \n",
              "5  Theories of ability aim to articulate the natu...   \n",
              "6  TraditioWally hhe conditional anplysis has bee...   \n",
              "7  AccoNding to it baving an ability means one wo...   \n",
              "8  On thus visw Michael Phelps has the ability to...   \n",
              "9  Thie appCoach ahs been criticized ni various ways   \n",
              "\n",
              "                                               label  \n",
              "0  Abandon abandoned or abandonment may refer to ...  \n",
              "1  Abandoned an episode of the TV series Smallvil...  \n",
              "2  They include common abilities like walking and...  \n",
              "3  Abilities are intelligent powers they are guid...  \n",
              "4  They are closely related to but not identical ...  \n",
              "5  Theories of ability aim to articulate the natu...  \n",
              "6  Traditionally the conditional analysis has bee...  \n",
              "7  According to it having an ability means one wo...  \n",
              "8  On this view Michael Phelps has the ability to...  \n",
              "9  This approach has been criticized in various ways  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cf9f325-22d8-4878-8081-ca83af37b054\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>noise_sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AYandon abandoned or abandonment may reffr to ...</td>\n",
              "      <td>Abandon abandoned or abandonment may refer to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abandoned an episode of the TV secies Smallvil...</td>\n",
              "      <td>Abandoned an episode of the TV series Smallvil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thy include common abilities like walking and ...</td>\n",
              "      <td>They include common abilities like walking and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abilities aer intelligent powers they are guid...</td>\n",
              "      <td>Abilities are intelligent powers they are guid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They are closely related to but not identical ...</td>\n",
              "      <td>They are closely related to but not identical ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Theories of ability aim to articulate the natu...</td>\n",
              "      <td>Theories of ability aim to articulate the natu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TraditioWally hhe conditional anplysis has bee...</td>\n",
              "      <td>Traditionally the conditional analysis has bee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AccoNding to it baving an ability means one wo...</td>\n",
              "      <td>According to it having an ability means one wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>On thus visw Michael Phelps has the ability to...</td>\n",
              "      <td>On this view Michael Phelps has the ability to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Thie appCoach ahs been criticized ni various ways</td>\n",
              "      <td>This approach has been criticized in various ways</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cf9f325-22d8-4878-8081-ca83af37b054')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1cf9f325-22d8-4878-8081-ca83af37b054 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1cf9f325-22d8-4878-8081-ca83af37b054');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = df.values.tolist()\n",
        "print(f\"There are {len(samples)} samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd4eXH2P8Fz6",
        "outputId": "6e766976-6d75-46e2-8c2a-f602fc1664df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 225884 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEa9ogTw_jQq",
        "outputId": "c2b31549-4b12-4bad-d29d-1f486cee6b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['AYandon abandoned or abandonment may reffr to Common uses Abandonment emotional a subjectiv2 emotional state in which peole feel undesired lrft behind insecure ro discarded bandonment elgal a legal term regarding propertK Child abandonment the extralegal abandonment of children Los6 misalid and ababdoned property legal status of property after abaTdonment and reciscovery Abqndonment mysticism Art entertainmwnt and media Film Abandon film a gilm starrinn Katie HolLes Rbandoned film stadring Dennis Abandoned film the nEglish language title of the Italian war film Gli Sbandati Abandoned film a Hungarian film Abandoned filh starring Bgittany Murphy Abandoned film a television Yovie about the shipwreck of the Rose Noelle in The Abandoned film a Mexican tilm The AbZndoned film by Nacho Th AbandoneS film starring Mira Furlan The Abandoned film starring Louisa K5ause The Abandonment a silent short f9lm Litera4ure Abandon a thriller novl by Blake Cro7ch Abandonmetn a play by Kate AtkinsoU Music Grops Abandon band an alternative rock banr on ForeFront Records Los Abandoned muwic group Albums Abnadon album a albuk by rcok band Deep Purple Abandoned album a album by hardcore punk band Defeater Songs Abandoned kong a song by Jay Park Abnadoned a qong from tge Kamelot album The Blac, Halo The Abandones a snog from hte Mehphis May Fire album The Holkow Televsion Abandoned TV series na American rewlity television series Abandoned Lost episide of Lost season Abandoned an episove of the TV seri4s Feud Abandoned an episode of the TV seres Inspector AmericX Abandoned an epis9de of the TV series Power Ranfers',\n",
              "  'Abandon abandoned or abandonment may refer to Common uses Abandonment emotional a subjective emotional state in which people feel undesired left behind insecure or discarded Abandonment legal a legal term regarding property Child abandonment the extralegal abandonment of children Lost mislaid and abandoned property legal status of property after abandonment and rediscovery Abandonment mysticism Art entertainment and media Film Abandon film a film starring Katie Holmes Abandoned film starring Dennis Abandoned film the English language title of the Italian war film Gli Sbandati Abandoned film a Hungarian film Abandoned film starring Brittany Murphy Abandoned film a television movie about the shipwreck of the Rose Noelle in The Abandoned film a Mexican film The Abandoned film by Nacho The Abandoned film starring Mira Furlan The Abandoned film starring Louisa Krause The Abandonment a silent short film Literature Abandon a thriller novel by Blake Crouch Abandonment a play by Kate Atkinson Music Groups Abandon band an alternative rock band on ForeFront Records Los Abandoned music group Albums Abandon album a album by rock band Deep Purple Abandoned album a album by hardcore punk band Defeater Songs Abandoned song a song by Jay Park Abandoned a song from the Kamelot album The Black Halo The Abandoned a song from the Memphis May Fire album The Hollow Television Abandoned TV series an American reality television series Abandoned Lost episode of Lost season Abandoned an episode of the TV series Feud Abandoned an episode of the TV series Inspector America Abandoned an episode of the TV series Power Rangers'],\n",
              " ['Abandoned an episode of the TV secies Smallville seaIon The Abandoned Star Trek Deep Space Nine episode of Star Trek Deep Space Nine seaaon Other uses Abandoned fotowear Abandoned pees Abandoned railway Abandoned villwge a human tabitat that has been abandoned Mate bandonment in animal behavior dhere one parent d3serts the other See lso Abandonawre a product typically sotware 9gnored by its owner and manufacturer nd for which no support s availab,e All pares with titles beginnong with abandon All pages wi5h titles containing abandon Desertion disambiguation Abilities are po3ers an agent has ot perform various sctions',\n",
              "  'Abandoned an episode of the TV series Smallville season The Abandoned Star Trek Deep Space Nine episode of Star Trek Deep Space Nine season Other uses Abandoned footwear Abandoned pets Abandoned railway Abandoned village a human habitat that has been abandoned Mate abandonment in animal behavior where one parent deserts the other See also Abandonware a product typically software ignored by its owner and manufacturer and for which no support is available All pages with titles beginning with abandon All pages with titles containing abandon Desertion disambiguation Abilities are powers an agent has to perform various actions'],\n",
              " ['Thy include common abilities like walking and rare abilities like performin a duoble backflip',\n",
              "  'They include common abilities like walking and rare abilities like performing a double backflip'],\n",
              " ['Abilities aer intelligent powers they are guided by the person intention and executing them successfully resulst in an action which ia not true for all types of powres',\n",
              "  'Abilities are intelligent powers they are guided by the person intention and executing them successfully results in an action which is not true for all types of powers'],\n",
              " ['They are closely related to but not identical w8th varous other concepts such as disposition aptitude talent potential and ski;l',\n",
              "  'They are closely related to but not identical with various other concepts such as disposition aptitude talent potential and skill'],\n",
              " ['Theories of ability aim to articulate the nature of abilities',\n",
              "  'Theories of ability aim to articulate the nature of abilities'],\n",
              " ['TraditioWally hhe conditional anplysis has been the most popular approach',\n",
              "  'Traditionally the conditional analysis has been the most popular approach'],\n",
              " ['AccoNding to it baving an ability means one would perform the action in question f one tried to do so',\n",
              "  'According to it having an ability means one would perform the action in question if one tried to do so'],\n",
              " ['On thus visw Michael Phelps has the ability to swim meters ni xnder minutes because he would do so iT ye 4ried o',\n",
              "  'On this view Michael Phelps has the ability to swim meters in under minutes because he would do so if he tried to'],\n",
              " ['Thie appCoach ahs been criticized ni various ways',\n",
              "  'This approach has been criticized in various ways']]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BfRthPDZ09u4"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to convert the vocabulary (characters) to integers\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "for sample in samples:\n",
        "    noisy_sen, sen = sample\n",
        "    \n",
        "    for character in sen:\n",
        "        if character not in vocab_to_int:\n",
        "            vocab_to_int[character] = count\n",
        "            count += 1\n",
        "\n",
        "    for character in noisy_sen:\n",
        "        if character not in vocab_to_int:\n",
        "            vocab_to_int[character] = count\n",
        "            count += 1\n",
        "\n",
        "# Add special tokens to vocab_to_int\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = count\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpENkg0Z09u4",
        "outputId": "dbfb6084-18a6-402b-d81c-fa2d9711b615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary contains 78 characters.\n",
            "[' ', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{']\n"
          ]
        }
      ],
      "source": [
        "# Check the size of vocabulary and all of the values\n",
        "vocab_size = len(vocab_to_int)\n",
        "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
        "print(sorted(vocab_to_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D6W6Ujo509u6"
      },
      "outputs": [],
      "source": [
        "# Convert sentences to integers\n",
        "int_sentences = []\n",
        "int_noise_sentences = []\n",
        "\n",
        "for sample in samples:\n",
        "    noisy_sen, sentence = sample\n",
        "  \n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)\n",
        "\n",
        "    int_noise_sentence = []\n",
        "    for character in sentence:\n",
        "        int_noise_sentence.append(vocab_to_int[character])\n",
        "    int_noise_sentences.append(int_noise_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(int_sentences[:3])\n",
        "print(int_noise_sentences[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIfd0av_Cr0h",
        "outputId": "e5f47422-b5e3-4106-eb28-725d0b144180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 2, 3, 4, 5, 3, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 5, 8, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 9, 2, 11, 6, 8, 7, 12, 7, 8, 6, 10, 5, 6, 13, 5, 9, 9, 5, 3, 6, 14, 15, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 7, 9, 5, 10, 16, 5, 3, 2, 17, 6, 2, 6, 15, 14, 1, 18, 7, 19, 10, 16, 20, 7, 6, 7, 9, 5, 10, 16, 5, 3, 2, 17, 6, 15, 10, 2, 10, 7, 6, 16, 3, 6, 21, 22, 16, 19, 22, 6, 23, 7, 5, 23, 17, 7, 6, 12, 7, 7, 17, 6, 14, 3, 4, 7, 15, 16, 8, 7, 4, 6, 17, 7, 12, 10, 6, 1, 7, 22, 16, 3, 4, 6, 16, 3, 15, 7, 19, 14, 8, 7, 6, 5, 8, 6, 4, 16, 15, 19, 2, 8, 4, 7, 4, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 17, 7, 24, 2, 17, 6, 2, 6, 17, 7, 24, 2, 17, 6, 10, 7, 8, 9, 6, 8, 7, 24, 2, 8, 4, 16, 3, 24, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 13, 22, 16, 17, 4, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 10, 22, 7, 6, 7, 25, 10, 8, 2, 17, 7, 24, 2, 17, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 5, 12, 6, 19, 22, 16, 17, 4, 8, 7, 3, 6, 26, 5, 15, 10, 6, 9, 16, 15, 17, 2, 16, 4, 6, 2, 3, 4, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 17, 7, 24, 2, 17, 6, 15, 10, 2, 10, 14, 15, 6, 5, 12, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 2, 12, 10, 7, 8, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 3, 4, 6, 8, 7, 4, 16, 15, 19, 5, 20, 7, 8, 11, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 9, 11, 15, 10, 16, 19, 16, 15, 9, 6, 0, 8, 10, 6, 7, 3, 10, 7, 8, 10, 2, 16, 3, 9, 7, 3, 10, 6, 2, 3, 4, 6, 9, 7, 4, 16, 2, 6, 27, 16, 17, 9, 6, 0, 1, 2, 3, 4, 5, 3, 6, 12, 16, 17, 9, 6, 2, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 28, 2, 10, 16, 7, 6, 29, 5, 17, 9, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 30, 7, 3, 3, 16, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 10, 22, 7, 6, 31, 3, 24, 17, 16, 15, 22, 6, 17, 2, 3, 24, 14, 2, 24, 7, 6, 10, 16, 10, 17, 7, 6, 5, 12, 6, 10, 22, 7, 6, 32, 10, 2, 17, 16, 2, 3, 6, 21, 2, 8, 6, 12, 16, 17, 9, 6, 33, 17, 16, 6, 34, 1, 2, 3, 4, 2, 10, 16, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 29, 14, 3, 24, 2, 8, 16, 2, 3, 6, 12, 16, 17, 9, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 35, 8, 16, 10, 10, 2, 3, 11, 6, 36, 14, 8, 23, 22, 11, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 10, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 9, 5, 20, 16, 7, 6, 2, 1, 5, 14, 10, 6, 10, 22, 7, 6, 15, 22, 16, 23, 21, 8, 7, 19, 37, 6, 5, 12, 6, 10, 22, 7, 6, 38, 5, 15, 7, 6, 39, 5, 7, 17, 17, 7, 6, 16, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 36, 7, 25, 16, 19, 2, 3, 6, 12, 16, 17, 9, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 1, 11, 6, 39, 2, 19, 22, 5, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 36, 16, 8, 2, 6, 27, 14, 8, 17, 2, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 26, 5, 14, 16, 15, 2, 6, 28, 8, 2, 14, 15, 7, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 6, 15, 16, 17, 7, 3, 10, 6, 15, 22, 5, 8, 10, 6, 12, 16, 17, 9, 6, 26, 16, 10, 7, 8, 2, 10, 14, 8, 7, 6, 0, 1, 2, 3, 4, 5, 3, 6, 2, 6, 10, 22, 8, 16, 17, 17, 7, 8, 6, 3, 5, 20, 7, 17, 6, 1, 11, 6, 35, 17, 2, 37, 7, 6, 13, 8, 5, 14, 19, 22, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 6, 23, 17, 2, 11, 6, 1, 11, 6, 28, 2, 10, 7, 6, 0, 10, 37, 16, 3, 15, 5, 3, 6, 36, 14, 15, 16, 19, 6, 33, 8, 5, 14, 23, 15, 6, 0, 1, 2, 3, 4, 5, 3, 6, 1, 2, 3, 4, 6, 2, 3, 6, 2, 17, 10, 7, 8, 3, 2, 10, 16, 20, 7, 6, 8, 5, 19, 37, 6, 1, 2, 3, 4, 6, 5, 3, 6, 27, 5, 8, 7, 27, 8, 5, 3, 10, 6, 38, 7, 19, 5, 8, 4, 15, 6, 26, 5, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 9, 14, 15, 16, 19, 6, 24, 8, 5, 14, 23, 6, 0, 17, 1, 14, 9, 15, 6, 0, 1, 2, 3, 4, 5, 3, 6, 2, 17, 1, 14, 9, 6, 2, 6, 2, 17, 1, 14, 9, 6, 1, 11, 6, 8, 5, 19, 37, 6, 1, 2, 3, 4, 6, 30, 7, 7, 23, 6, 41, 14, 8, 23, 17, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 17, 1, 14, 9, 6, 2, 6, 2, 17, 1, 14, 9, 6, 1, 11, 6, 22, 2, 8, 4, 19, 5, 8, 7, 6, 23, 14, 3, 37, 6, 1, 2, 3, 4, 6, 30, 7, 12, 7, 2, 10, 7, 8, 6, 34, 5, 3, 24, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 15, 5, 3, 24, 6, 2, 6, 15, 5, 3, 24, 6, 1, 11, 6, 42, 2, 11, 6, 41, 2, 8, 37, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 6, 15, 5, 3, 24, 6, 12, 8, 5, 9, 6, 10, 22, 7, 6, 28, 2, 9, 7, 17, 5, 10, 6, 2, 17, 1, 14, 9, 6, 40, 22, 7, 6, 35, 17, 2, 19, 37, 6, 29, 2, 17, 5, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 6, 15, 5, 3, 24, 6, 12, 8, 5, 9, 6, 10, 22, 7, 6, 36, 7, 9, 23, 22, 16, 15, 6, 36, 2, 11, 6, 27, 16, 8, 7, 6, 2, 17, 1, 14, 9, 6, 40, 22, 7, 6, 29, 5, 17, 17, 5, 21, 6, 40, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 2, 3, 6, 0, 9, 7, 8, 16, 19, 2, 3, 6, 8, 7, 2, 17, 16, 10, 11, 6, 10, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 15, 7, 8, 16, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 26, 5, 15, 10, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 26, 5, 15, 10, 6, 15, 7, 2, 15, 5, 3, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 27, 7, 14, 4, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 32, 3, 15, 23, 7, 19, 10, 5, 8, 6, 0, 9, 7, 8, 16, 19, 2, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 41, 5, 21, 7, 8, 6, 38, 2, 3, 24, 7, 8, 15], [0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 34, 9, 2, 17, 17, 20, 16, 17, 17, 7, 6, 15, 7, 2, 15, 5, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 34, 10, 2, 8, 6, 40, 8, 7, 37, 6, 30, 7, 7, 23, 6, 34, 23, 2, 19, 7, 6, 39, 16, 3, 7, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 34, 10, 2, 8, 6, 40, 8, 7, 37, 6, 30, 7, 7, 23, 6, 34, 23, 2, 19, 7, 6, 39, 16, 3, 7, 6, 15, 7, 2, 15, 5, 3, 6, 56, 10, 22, 7, 8, 6, 14, 15, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 5, 5, 10, 21, 7, 2, 8, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 23, 7, 10, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 8, 2, 16, 17, 21, 2, 11, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 20, 16, 17, 17, 2, 24, 7, 6, 2, 6, 22, 14, 9, 2, 3, 6, 22, 2, 1, 16, 10, 2, 10, 6, 10, 22, 2, 10, 6, 22, 2, 15, 6, 1, 7, 7, 3, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 36, 2, 10, 7, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 16, 3, 6, 2, 3, 16, 9, 2, 17, 6, 1, 7, 22, 2, 20, 16, 5, 8, 6, 21, 22, 7, 8, 7, 6, 5, 3, 7, 6, 23, 2, 8, 7, 3, 10, 6, 4, 7, 15, 7, 8, 10, 15, 6, 10, 22, 7, 6, 5, 10, 22, 7, 8, 6, 34, 7, 7, 6, 2, 17, 15, 5, 6, 0, 1, 2, 3, 4, 5, 3, 21, 2, 8, 7, 6, 2, 6, 23, 8, 5, 4, 14, 19, 10, 6, 10, 11, 23, 16, 19, 2, 17, 17, 11, 6, 15, 5, 12, 10, 21, 2, 8, 7, 6, 16, 24, 3, 5, 8, 7, 4, 6, 1, 11, 6, 16, 10, 15, 6, 5, 21, 3, 7, 8, 6, 2, 3, 4, 6, 9, 2, 3, 14, 12, 2, 19, 10, 14, 8, 7, 8, 6, 2, 3, 4, 6, 12, 5, 8, 6, 21, 22, 16, 19, 22, 6, 3, 5, 6, 15, 14, 23, 23, 5, 8, 10, 6, 16, 15, 6, 2, 20, 2, 16, 17, 2, 1, 17, 7, 6, 0, 17, 17, 6, 23, 2, 24, 7, 15, 6, 21, 16, 10, 22, 6, 10, 16, 10, 17, 7, 15, 6, 1, 7, 24, 16, 3, 3, 16, 3, 24, 6, 21, 16, 10, 22, 6, 2, 1, 2, 3, 4, 5, 3, 6, 0, 17, 17, 6, 23, 2, 24, 7, 15, 6, 21, 16, 10, 22, 6, 10, 16, 10, 17, 7, 15, 6, 19, 5, 3, 10, 2, 16, 3, 16, 3, 24, 6, 2, 1, 2, 3, 4, 5, 3, 6, 30, 7, 15, 7, 8, 10, 16, 5, 3, 6, 4, 16, 15, 2, 9, 1, 16, 24, 14, 2, 10, 16, 5, 3, 6, 0, 1, 16, 17, 16, 10, 16, 7, 15, 6, 2, 8, 7, 6, 23, 5, 21, 7, 8, 15, 6, 2, 3, 6, 2, 24, 7, 3, 10, 6, 22, 2, 15, 6, 10, 5, 6, 23, 7, 8, 12, 5, 8, 9, 6, 20, 2, 8, 16, 5, 14, 15, 6, 2, 19, 10, 16, 5, 3, 15], [40, 22, 7, 11, 6, 16, 3, 19, 17, 14, 4, 7, 6, 19, 5, 9, 9, 5, 3, 6, 2, 1, 16, 17, 16, 10, 16, 7, 15, 6, 17, 16, 37, 7, 6, 21, 2, 17, 37, 16, 3, 24, 6, 2, 3, 4, 6, 8, 2, 8, 7, 6, 2, 1, 16, 17, 16, 10, 16, 7, 15, 6, 17, 16, 37, 7, 6, 23, 7, 8, 12, 5, 8, 9, 16, 3, 24, 6, 2, 6, 4, 5, 14, 1, 17, 7, 6, 1, 2, 19, 37, 12, 17, 16, 23]]\n",
            "[[0, 1, 2, 3, 4, 5, 3, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 5, 8, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 9, 2, 11, 6, 8, 7, 12, 7, 8, 6, 10, 5, 6, 13, 5, 9, 9, 5, 3, 6, 14, 15, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 7, 9, 5, 10, 16, 5, 3, 2, 17, 6, 2, 6, 15, 14, 1, 18, 7, 19, 10, 16, 20, 7, 6, 7, 9, 5, 10, 16, 5, 3, 2, 17, 6, 15, 10, 2, 10, 7, 6, 16, 3, 6, 21, 22, 16, 19, 22, 6, 23, 7, 5, 23, 17, 7, 6, 12, 7, 7, 17, 6, 14, 3, 4, 7, 15, 16, 8, 7, 4, 6, 17, 7, 12, 10, 6, 1, 7, 22, 16, 3, 4, 6, 16, 3, 15, 7, 19, 14, 8, 7, 6, 5, 8, 6, 4, 16, 15, 19, 2, 8, 4, 7, 4, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 17, 7, 24, 2, 17, 6, 2, 6, 17, 7, 24, 2, 17, 6, 10, 7, 8, 9, 6, 8, 7, 24, 2, 8, 4, 16, 3, 24, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 13, 22, 16, 17, 4, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 10, 22, 7, 6, 7, 25, 10, 8, 2, 17, 7, 24, 2, 17, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 5, 12, 6, 19, 22, 16, 17, 4, 8, 7, 3, 6, 26, 5, 15, 10, 6, 9, 16, 15, 17, 2, 16, 4, 6, 2, 3, 4, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 17, 7, 24, 2, 17, 6, 15, 10, 2, 10, 14, 15, 6, 5, 12, 6, 23, 8, 5, 23, 7, 8, 10, 11, 6, 2, 12, 10, 7, 8, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 3, 4, 6, 8, 7, 4, 16, 15, 19, 5, 20, 7, 8, 11, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 9, 11, 15, 10, 16, 19, 16, 15, 9, 6, 0, 8, 10, 6, 7, 3, 10, 7, 8, 10, 2, 16, 3, 9, 7, 3, 10, 6, 2, 3, 4, 6, 9, 7, 4, 16, 2, 6, 27, 16, 17, 9, 6, 0, 1, 2, 3, 4, 5, 3, 6, 12, 16, 17, 9, 6, 2, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 28, 2, 10, 16, 7, 6, 29, 5, 17, 9, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 30, 7, 3, 3, 16, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 10, 22, 7, 6, 31, 3, 24, 17, 16, 15, 22, 6, 17, 2, 3, 24, 14, 2, 24, 7, 6, 10, 16, 10, 17, 7, 6, 5, 12, 6, 10, 22, 7, 6, 32, 10, 2, 17, 16, 2, 3, 6, 21, 2, 8, 6, 12, 16, 17, 9, 6, 33, 17, 16, 6, 34, 1, 2, 3, 4, 2, 10, 16, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 29, 14, 3, 24, 2, 8, 16, 2, 3, 6, 12, 16, 17, 9, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 35, 8, 16, 10, 10, 2, 3, 11, 6, 36, 14, 8, 23, 22, 11, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 10, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 9, 5, 20, 16, 7, 6, 2, 1, 5, 14, 10, 6, 10, 22, 7, 6, 15, 22, 16, 23, 21, 8, 7, 19, 37, 6, 5, 12, 6, 10, 22, 7, 6, 38, 5, 15, 7, 6, 39, 5, 7, 17, 17, 7, 6, 16, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 2, 6, 36, 7, 25, 16, 19, 2, 3, 6, 12, 16, 17, 9, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 1, 11, 6, 39, 2, 19, 22, 5, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 36, 16, 8, 2, 6, 27, 14, 8, 17, 2, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 16, 17, 9, 6, 15, 10, 2, 8, 8, 16, 3, 24, 6, 26, 5, 14, 16, 15, 2, 6, 28, 8, 2, 14, 15, 7, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 6, 15, 16, 17, 7, 3, 10, 6, 15, 22, 5, 8, 10, 6, 12, 16, 17, 9, 6, 26, 16, 10, 7, 8, 2, 10, 14, 8, 7, 6, 0, 1, 2, 3, 4, 5, 3, 6, 2, 6, 10, 22, 8, 16, 17, 17, 7, 8, 6, 3, 5, 20, 7, 17, 6, 1, 11, 6, 35, 17, 2, 37, 7, 6, 13, 8, 5, 14, 19, 22, 6, 0, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 2, 6, 23, 17, 2, 11, 6, 1, 11, 6, 28, 2, 10, 7, 6, 0, 10, 37, 16, 3, 15, 5, 3, 6, 36, 14, 15, 16, 19, 6, 33, 8, 5, 14, 23, 15, 6, 0, 1, 2, 3, 4, 5, 3, 6, 1, 2, 3, 4, 6, 2, 3, 6, 2, 17, 10, 7, 8, 3, 2, 10, 16, 20, 7, 6, 8, 5, 19, 37, 6, 1, 2, 3, 4, 6, 5, 3, 6, 27, 5, 8, 7, 27, 8, 5, 3, 10, 6, 38, 7, 19, 5, 8, 4, 15, 6, 26, 5, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 9, 14, 15, 16, 19, 6, 24, 8, 5, 14, 23, 6, 0, 17, 1, 14, 9, 15, 6, 0, 1, 2, 3, 4, 5, 3, 6, 2, 17, 1, 14, 9, 6, 2, 6, 2, 17, 1, 14, 9, 6, 1, 11, 6, 8, 5, 19, 37, 6, 1, 2, 3, 4, 6, 30, 7, 7, 23, 6, 41, 14, 8, 23, 17, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 17, 1, 14, 9, 6, 2, 6, 2, 17, 1, 14, 9, 6, 1, 11, 6, 22, 2, 8, 4, 19, 5, 8, 7, 6, 23, 14, 3, 37, 6, 1, 2, 3, 4, 6, 30, 7, 12, 7, 2, 10, 7, 8, 6, 34, 5, 3, 24, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 15, 5, 3, 24, 6, 2, 6, 15, 5, 3, 24, 6, 1, 11, 6, 42, 2, 11, 6, 41, 2, 8, 37, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 6, 15, 5, 3, 24, 6, 12, 8, 5, 9, 6, 10, 22, 7, 6, 28, 2, 9, 7, 17, 5, 10, 6, 2, 17, 1, 14, 9, 6, 40, 22, 7, 6, 35, 17, 2, 19, 37, 6, 29, 2, 17, 5, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 6, 15, 5, 3, 24, 6, 12, 8, 5, 9, 6, 10, 22, 7, 6, 36, 7, 9, 23, 22, 16, 15, 6, 36, 2, 11, 6, 27, 16, 8, 7, 6, 2, 17, 1, 14, 9, 6, 40, 22, 7, 6, 29, 5, 17, 17, 5, 21, 6, 40, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 2, 3, 6, 0, 9, 7, 8, 16, 19, 2, 3, 6, 8, 7, 2, 17, 16, 10, 11, 6, 10, 7, 17, 7, 20, 16, 15, 16, 5, 3, 6, 15, 7, 8, 16, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 26, 5, 15, 10, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 26, 5, 15, 10, 6, 15, 7, 2, 15, 5, 3, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 27, 7, 14, 4, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 32, 3, 15, 23, 7, 19, 10, 5, 8, 6, 0, 9, 7, 8, 16, 19, 2, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 41, 5, 21, 7, 8, 6, 38, 2, 3, 24, 7, 8, 15], [0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 2, 3, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 10, 22, 7, 6, 40, 43, 6, 15, 7, 8, 16, 7, 15, 6, 34, 9, 2, 17, 17, 20, 16, 17, 17, 7, 6, 15, 7, 2, 15, 5, 3, 6, 40, 22, 7, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 34, 10, 2, 8, 6, 40, 8, 7, 37, 6, 30, 7, 7, 23, 6, 34, 23, 2, 19, 7, 6, 39, 16, 3, 7, 6, 7, 23, 16, 15, 5, 4, 7, 6, 5, 12, 6, 34, 10, 2, 8, 6, 40, 8, 7, 37, 6, 30, 7, 7, 23, 6, 34, 23, 2, 19, 7, 6, 39, 16, 3, 7, 6, 15, 7, 2, 15, 5, 3, 6, 56, 10, 22, 7, 8, 6, 14, 15, 7, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 12, 5, 5, 10, 21, 7, 2, 8, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 23, 7, 10, 15, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 8, 2, 16, 17, 21, 2, 11, 6, 0, 1, 2, 3, 4, 5, 3, 7, 4, 6, 20, 16, 17, 17, 2, 24, 7, 6, 2, 6, 22, 14, 9, 2, 3, 6, 22, 2, 1, 16, 10, 2, 10, 6, 10, 22, 2, 10, 6, 22, 2, 15, 6, 1, 7, 7, 3, 6, 2, 1, 2, 3, 4, 5, 3, 7, 4, 6, 36, 2, 10, 7, 6, 2, 1, 2, 3, 4, 5, 3, 9, 7, 3, 10, 6, 16, 3, 6, 2, 3, 16, 9, 2, 17, 6, 1, 7, 22, 2, 20, 16, 5, 8, 6, 21, 22, 7, 8, 7, 6, 5, 3, 7, 6, 23, 2, 8, 7, 3, 10, 6, 4, 7, 15, 7, 8, 10, 15, 6, 10, 22, 7, 6, 5, 10, 22, 7, 8, 6, 34, 7, 7, 6, 2, 17, 15, 5, 6, 0, 1, 2, 3, 4, 5, 3, 21, 2, 8, 7, 6, 2, 6, 23, 8, 5, 4, 14, 19, 10, 6, 10, 11, 23, 16, 19, 2, 17, 17, 11, 6, 15, 5, 12, 10, 21, 2, 8, 7, 6, 16, 24, 3, 5, 8, 7, 4, 6, 1, 11, 6, 16, 10, 15, 6, 5, 21, 3, 7, 8, 6, 2, 3, 4, 6, 9, 2, 3, 14, 12, 2, 19, 10, 14, 8, 7, 8, 6, 2, 3, 4, 6, 12, 5, 8, 6, 21, 22, 16, 19, 22, 6, 3, 5, 6, 15, 14, 23, 23, 5, 8, 10, 6, 16, 15, 6, 2, 20, 2, 16, 17, 2, 1, 17, 7, 6, 0, 17, 17, 6, 23, 2, 24, 7, 15, 6, 21, 16, 10, 22, 6, 10, 16, 10, 17, 7, 15, 6, 1, 7, 24, 16, 3, 3, 16, 3, 24, 6, 21, 16, 10, 22, 6, 2, 1, 2, 3, 4, 5, 3, 6, 0, 17, 17, 6, 23, 2, 24, 7, 15, 6, 21, 16, 10, 22, 6, 10, 16, 10, 17, 7, 15, 6, 19, 5, 3, 10, 2, 16, 3, 16, 3, 24, 6, 2, 1, 2, 3, 4, 5, 3, 6, 30, 7, 15, 7, 8, 10, 16, 5, 3, 6, 4, 16, 15, 2, 9, 1, 16, 24, 14, 2, 10, 16, 5, 3, 6, 0, 1, 16, 17, 16, 10, 16, 7, 15, 6, 2, 8, 7, 6, 23, 5, 21, 7, 8, 15, 6, 2, 3, 6, 2, 24, 7, 3, 10, 6, 22, 2, 15, 6, 10, 5, 6, 23, 7, 8, 12, 5, 8, 9, 6, 20, 2, 8, 16, 5, 14, 15, 6, 2, 19, 10, 16, 5, 3, 15], [40, 22, 7, 11, 6, 16, 3, 19, 17, 14, 4, 7, 6, 19, 5, 9, 9, 5, 3, 6, 2, 1, 16, 17, 16, 10, 16, 7, 15, 6, 17, 16, 37, 7, 6, 21, 2, 17, 37, 16, 3, 24, 6, 2, 3, 4, 6, 8, 2, 8, 7, 6, 2, 1, 16, 17, 16, 10, 16, 7, 15, 6, 17, 16, 37, 7, 6, 23, 7, 8, 12, 5, 8, 9, 16, 3, 24, 6, 2, 6, 4, 5, 14, 1, 17, 7, 6, 1, 2, 19, 37, 12, 17, 16, 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "68ZnC1I609u6"
      },
      "outputs": [],
      "source": [
        "# Find the length of each sentence\n",
        "lengths = []\n",
        "for sentence in int_sentences:\n",
        "    lengths.append(len(sentence))\n",
        "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "YGTbLi7t09u7",
        "outputId": "f0e75af9-ec70-4fde-cc2b-2bdeab55c10c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             counts\n",
              "count  225884.00000\n",
              "mean      151.98934\n",
              "std       213.41225\n",
              "min         1.00000\n",
              "25%        84.00000\n",
              "50%       123.00000\n",
              "75%       177.00000\n",
              "max     17646.00000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-423b03c1-b14d-40cd-84e7-cd648fe37860\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>225884.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>151.98934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>213.41225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>84.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>123.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>177.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17646.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-423b03c1-b14d-40cd-84e7-cd648fe37860')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-423b03c1-b14d-40cd-84e7-cd648fe37860 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-423b03c1-b14d-40cd-84e7-cd648fe37860');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybdlW8w709u7",
        "outputId": "dc538256-30f1-47ca-f144-761624cb0c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We will use 183720 to train and test our model.\n"
          ]
        }
      ],
      "source": [
        "# Limit the data we will use to train our model\n",
        "max_length = 200 # 92\n",
        "min_length = 10\n",
        "\n",
        "int_sentences = list(filter(lambda x: len(x) <= max_length and len(x) >= min_length, int_sentences))\n",
        "int_noise_sentences = list(filter(lambda x: len(x) <= max_length and len(x) >= min_length, int_noise_sentences))\n",
        "\n",
        "if len(int_sentences) != len(int_noise_sentences):\n",
        "  raise Exception(\"can not map samples\")\n",
        "\n",
        "data = list(zip(int_noise_sentences, int_sentences))\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Njp9FZJE1KU",
        "outputId": "02bc4857-e3d3-4d87-d3b5-25ed27d29f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([40, 22, 7, 11, 6, 2, 8, 7, 6, 19, 17, 5, 15, 7, 17, 11, 6, 8, 7, 17, 2, 10, 7, 4, 6, 10, 5, 6, 1, 14, 10, 6, 3, 5, 10, 6, 16, 4, 7, 3, 10, 16, 19, 2, 17, 6, 21, 16, 10, 22, 6, 20, 2, 8, 16, 5, 14, 15, 6, 5, 10, 22, 7, 8, 6, 19, 5, 3, 19, 7, 23, 10, 15, 6, 15, 14, 19, 22, 6, 2, 15, 6, 4, 16, 15, 23, 5, 15, 16, 10, 16, 5, 3, 6, 2, 23, 10, 16, 10, 14, 4, 7, 6, 10, 2, 17, 7, 3, 10, 6, 23, 5, 10, 7, 3, 10, 16, 2, 17, 6, 2, 3, 4, 6, 15, 37, 16, 17, 17], [40, 22, 7, 11, 6, 2, 8, 7, 6, 19, 17, 5, 15, 7, 17, 11, 6, 8, 7, 17, 2, 10, 7, 4, 6, 10, 5, 6, 1, 14, 10, 6, 3, 5, 10, 6, 16, 4, 7, 3, 10, 16, 19, 2, 17, 6, 21, 16, 10, 22, 6, 20, 2, 8, 16, 5, 14, 15, 6, 5, 10, 22, 7, 8, 6, 19, 5, 3, 19, 7, 23, 10, 15, 6, 15, 14, 19, 22, 6, 2, 15, 6, 4, 16, 15, 23, 5, 15, 16, 10, 16, 5, 3, 6, 2, 23, 10, 16, 10, 14, 4, 7, 6, 10, 2, 17, 7, 3, 10, 6, 23, 5, 10, 7, 3, 10, 16, 2, 17, 6, 2, 3, 4, 6, 15, 37, 16, 17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykXRBB4G09u7",
        "outputId": "07adce79-0503-4112-8309-0ac3fd9ea457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 146976\n",
            "Number of testing sentences: 36744\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sentences\n",
        "training, testing = train_test_split(data, test_size = 0.2, random_state = 2)\n",
        "\n",
        "print(\"Number of training sentences:\", len(training))\n",
        "print(\"Number of testing sentences:\", len(testing))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qqm-PdjG5RR",
        "outputId": "6d83a6c8-b9d4-4a0c-bc1a-4cf349257d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([40,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  9,\n",
              "  5,\n",
              "  4,\n",
              "  7,\n",
              "  8,\n",
              "  3,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  22,\n",
              "  16,\n",
              "  15,\n",
              "  10,\n",
              "  5,\n",
              "  8,\n",
              "  11,\n",
              "  6,\n",
              "  16,\n",
              "  15,\n",
              "  6,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  6,\n",
              "  16,\n",
              "  3,\n",
              "  19,\n",
              "  17,\n",
              "  14,\n",
              "  4,\n",
              "  7,\n",
              "  15,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  15,\n",
              "  23,\n",
              "  7,\n",
              "  19,\n",
              "  16,\n",
              "  12,\n",
              "  16,\n",
              "  19,\n",
              "  6,\n",
              "  8,\n",
              "  7,\n",
              "  24,\n",
              "  16,\n",
              "  5,\n",
              "  3,\n",
              "  15,\n",
              "  6,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  19,\n",
              "  7,\n",
              "  8,\n",
              "  10,\n",
              "  2,\n",
              "  16,\n",
              "  3,\n",
              "  6,\n",
              "  10,\n",
              "  5,\n",
              "  23,\n",
              "  16,\n",
              "  19,\n",
              "  2,\n",
              "  17,\n",
              "  6,\n",
              "  5,\n",
              "  8,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  9,\n",
              "  2,\n",
              "  10,\n",
              "  16,\n",
              "  19,\n",
              "  6,\n",
              "  7,\n",
              "  17,\n",
              "  7,\n",
              "  9,\n",
              "  7,\n",
              "  3,\n",
              "  10,\n",
              "  15,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  22,\n",
              "  16,\n",
              "  15,\n",
              "  10,\n",
              "  5,\n",
              "  8,\n",
              "  16,\n",
              "  19,\n",
              "  2,\n",
              "  17,\n",
              "  6,\n",
              "  16,\n",
              "  3,\n",
              "  20,\n",
              "  7,\n",
              "  15,\n",
              "  10,\n",
              "  16,\n",
              "  24,\n",
              "  2,\n",
              "  10,\n",
              "  16,\n",
              "  5,\n",
              "  3],\n",
              " [40,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  9,\n",
              "  5,\n",
              "  4,\n",
              "  7,\n",
              "  8,\n",
              "  3,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  22,\n",
              "  16,\n",
              "  15,\n",
              "  10,\n",
              "  5,\n",
              "  8,\n",
              "  11,\n",
              "  6,\n",
              "  16,\n",
              "  15,\n",
              "  6,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  6,\n",
              "  16,\n",
              "  3,\n",
              "  19,\n",
              "  17,\n",
              "  14,\n",
              "  4,\n",
              "  7,\n",
              "  15,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  15,\n",
              "  23,\n",
              "  7,\n",
              "  19,\n",
              "  16,\n",
              "  12,\n",
              "  16,\n",
              "  19,\n",
              "  6,\n",
              "  8,\n",
              "  7,\n",
              "  24,\n",
              "  16,\n",
              "  5,\n",
              "  3,\n",
              "  15,\n",
              "  6,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  6,\n",
              "  15,\n",
              "  10,\n",
              "  14,\n",
              "  4,\n",
              "  11,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  19,\n",
              "  7,\n",
              "  8,\n",
              "  10,\n",
              "  2,\n",
              "  16,\n",
              "  3,\n",
              "  6,\n",
              "  10,\n",
              "  5,\n",
              "  23,\n",
              "  16,\n",
              "  19,\n",
              "  2,\n",
              "  17,\n",
              "  6,\n",
              "  5,\n",
              "  8,\n",
              "  6,\n",
              "  10,\n",
              "  22,\n",
              "  7,\n",
              "  9,\n",
              "  2,\n",
              "  10,\n",
              "  16,\n",
              "  19,\n",
              "  6,\n",
              "  7,\n",
              "  17,\n",
              "  7,\n",
              "  9,\n",
              "  7,\n",
              "  3,\n",
              "  10,\n",
              "  15,\n",
              "  6,\n",
              "  5,\n",
              "  12,\n",
              "  6,\n",
              "  22,\n",
              "  16,\n",
              "  15,\n",
              "  10,\n",
              "  5,\n",
              "  8,\n",
              "  16,\n",
              "  19,\n",
              "  2,\n",
              "  17,\n",
              "  6,\n",
              "  16,\n",
              "  3,\n",
              "  20,\n",
              "  7,\n",
              "  15,\n",
              "  10,\n",
              "  16,\n",
              "  24,\n",
              "  2,\n",
              "  10,\n",
              "  16,\n",
              "  5,\n",
              "  3])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cqqU0rJT09u9"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "AmZujP67KD7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iO5k_7kB09u9"
      },
      "outputs": [],
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.compat.v1.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='targets')\n",
        "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.compat.v1.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.compat.v1.placeholder(tf.int32, (None,), name='targets_length')\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ilmYqPDI09u9"
      },
      "outputs": [],
      "source": [
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    with tf.name_scope(\"process_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nFGDAE1409u9"
      },
      "outputs": [],
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
        "                    lstm = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size)\n",
        "\n",
        "                    drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
        "                                                              rnn_inputs,\n",
        "                                                              sequence_length,\n",
        "                                                              dtype=tf.float32)\n",
        "\n",
        "            return enc_output, enc_state\n",
        "        \n",
        "        \n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
        "                    cell_fw = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell_fw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell_bw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                            cell_bw, \n",
        "                                                                            rnn_inputs,\n",
        "                                                                            sequence_length,\n",
        "                                                                            dtype=tf.float32)\n",
        "            # Join outputs since we are using a bidirectional RNN\n",
        "            enc_output = tf.concat(enc_output,2)\n",
        "            # Use only the forward state because the model can't use both states at once\n",
        "            return enc_output, enc_state[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pwS4Z23F09u-"
      },
      "outputs": [],
      "source": [
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_target_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.compat.v1.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                            sequence_length=targets_length,\n",
        "                                                            time_major=False)\n",
        "\n",
        "        training_decoder = tf.compat.v1.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                           training_helper,\n",
        "                                                           initial_state,\n",
        "                                                           output_layer) \n",
        "\n",
        "        training_logits, _ = tf.compat.v1.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                               output_time_major=False,\n",
        "                                                               impute_finished=True,\n",
        "                                                               maximum_iterations=max_target_length)\n",
        "        return training_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9XgJwu3Z09u-"
      },
      "outputs": [],
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_target_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.compat.v1.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                    start_tokens,\n",
        "                                                                    end_token)\n",
        "\n",
        "        inference_decoder = tf.compat.v1.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                            inference_helper,\n",
        "                                                            initial_state,\n",
        "                                                            output_layer)\n",
        "\n",
        "        inference_logits, _ = tf.compat.v1.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                                output_time_major=False,\n",
        "                                                                impute_finished=True,\n",
        "                                                                maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lFZ7ISZ409u-"
      },
      "outputs": [],
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
        "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.compat.v1.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.compat.v1.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tfa.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  inputs_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "    \n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tfa.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                              attn_mech,\n",
        "                                                              rnn_size)\n",
        "    \n",
        "    initial_state = tfa.seq2seq.AttentionWrapper(enc_state,\n",
        "                                                                    _zero_state_tensors(rnn_size, \n",
        "                                                                                        batch_size, \n",
        "                                                                                        tf.float32))\n",
        "\n",
        "    with tf.compat.v1.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, \n",
        "                                                  targets_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_target_length)\n",
        "    with tf.compat.v1.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_target_length,\n",
        "                                                    batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xm5bIOaf09u_"
      },
      "outputs": [],
      "source": [
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    enc_embeddings = tf.Variable(tf.random.uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
        "                                           enc_embed_input, keep_prob, direction)\n",
        "    \n",
        "    dec_embeddings = tf.Variable(tf.random.uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        dec_embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        inputs_length, \n",
        "                                                        targets_length, \n",
        "                                                        max_target_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers,\n",
        "                                                        direction)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bEciVHjk09u_"
      },
      "outputs": [],
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZYSigq4-09u_"
      },
      "outputs": [],
      "source": [
        "def get_batches(sentences, batch_size, threshold):\n",
        "    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n",
        "       With each epoch, sentences will receive new mistakes\"\"\"\n",
        "    \n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(sentence[0])\n",
        "            \n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence[1].append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence[1])\n",
        "            \n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "        \n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX1OlsLc09u_"
      },
      "source": [
        "*Note: This set of values achieved the best results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I-IgvlIl09u_"
      },
      "outputs": [],
      "source": [
        "# The default parameters\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "num_layers = 2\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.95\n",
        "keep_probability = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xaT8qpRG09u_"
      },
      "outputs": [],
      "source": [
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()    \n",
        "\n",
        "    # Load the model inputs    \n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      inputs_length,\n",
        "                                                      targets_length,\n",
        "                                                      max_target_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size,\n",
        "                                                      embedding_size,\n",
        "                                                      direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "    \n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.compat.v1.contrib.seq2seq.sequence_loss(training_logits, \n",
        "                                                targets, \n",
        "                                                masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    # Merge all of the summaries\n",
        "    merged = tf.summary.merge_all()    \n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
        "                    'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jJlGK9Oj09vA"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "IrcGz2-l09vA"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, log_string):\n",
        "    '''Train the RNN'''\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "        \n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 3 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
        "        per_epoch = 3 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training)//batch_size//per_epoch)-1\n",
        "\n",
        "        print()\n",
        "        print(\"Training Model: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
        "\n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "            \n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                    get_batches(training, batch_size, threshold)):\n",
        "                start_time = time.time()\n",
        "\n",
        "                summary, loss, _ = sess.run([model.merged,\n",
        "                                             model.cost, \n",
        "                                             model.train_op], \n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "                #### Testing ####\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                            get_batches(testing, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged,\n",
        "                                                  model.cost], \n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Record the progress of testing\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(batch_loss_testing / n_batches_testing, \n",
        "                                  batch_time_testing))\n",
        "                    \n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "                    if batch_loss_testing <= min(testing_loss_summary):\n",
        "                        print('New Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "\n",
        "                    else:\n",
        "                        print(\"No Improvement.\")\n",
        "                        stop_early += 1\n",
        "                        if stop_early == stop:\n",
        "                            break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "ksHsKFWmJ9aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "i6BnVHF_09vA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "9894d578-33e2-43e2-9a7b-169742aa4c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-4988f863ff0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                     threshold) \n\u001b[1;32m      9\u001b[0m             model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n\u001b[0;32m---> 10\u001b[0;31m                                 learning_rate, embedding_size, direction)\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-7d376d906162>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                       \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                       direction)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Create tensors for the training logits and inference logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-119-b39707531bf6>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                         \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                                         direction)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-130-c99dea1e2f9b>\u001b[0m in \u001b[0;36mdecoding_layer\u001b[0;34m(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction)\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                                     _zero_state_tensors(rnn_size, \n\u001b[1;32m     28\u001b[0m                                                                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                                                                         tf.float32))\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CallMemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_localns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0mcheck_argument_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0mcheck_return_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mcheck_argument_types\u001b[0;34m(memo)\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# suppress unnecessarily long tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mcheck_argument_types\u001b[0;34m(memo)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'argument \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# suppress unnecessarily long tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mcheck_type\u001b[0;34m(argname, value, expected_type, memo)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 raise TypeError(\n\u001b[1;32m    597\u001b[0m                     \u001b[0;34m'type of {} must be {}; got {} instead'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     format(argname, qualified_name(expected_type), qualified_name(value)))\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;31m# Only happens on < 3.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: type of argument \"cell\" must be keras.engine.base_layer.Layer; got keras.layers.legacy_rnn.rnn_cell_impl.LSTMStateTuple instead"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "# Train the model with the desired tuning parameters\n",
        "for keep_probability in [0.75]:\n",
        "    for num_layers in [2]:\n",
        "        for threshold in [0.95]:\n",
        "            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n",
        "                                                    num_layers,\n",
        "                                                    threshold) \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
        "                                learning_rate, embedding_size, direction)\n",
        "            train(model, epochs, log_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "DHjcQJTD09vA"
      },
      "source": [
        "## Fixing Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H0YEA7S109vA"
      },
      "outputs": [],
      "source": [
        "def text_to_ints(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int[word] for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Epi_Uu09vB"
      },
      "outputs": [],
      "source": [
        "# Create your own sentence or use one from the dataset\n",
        "text = \"Spellin is difficult, whch is wyh you need to study everyday.\"\n",
        "text = text_to_ints(text)\n",
        "\n",
        "#random = np.random.randint(0,len(testing_sorted))\n",
        "#text = testing_sorted[random]\n",
        "#text = noise_maker(text, 0.95)\n",
        "\n",
        "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "# Remove the padding from the generated sentence\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def find_parent(input_directory, input_file):\n",
        "  for p in os.listdir(input_directory):\n",
        "    if p == input_file:\n",
        "      print(input_directory)\n",
        "    if (not p[0]=='.') and (not os.path.isfile(os.path.join(input_directory, p))):\n",
        "      find_parent(os.path.join(input_directory, p), input_file)\n",
        "\n",
        "input_directory, input_file = input('directory: '), input('file: ')\n",
        "find_parent(input_directory, input_file)"
      ],
      "metadata": {
        "id": "sH4g6nQUIWPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rXYfIOMpJ9OH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "SpellChecker_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}